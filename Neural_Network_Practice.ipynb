{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "[[3.957578 ]\n",
      " [1.1537654]\n",
      " [3.1674924]]\n"
     ]
    }
   ],
   "source": [
    "#complete forward propagation Example:\n",
    "import tensorflow as tf\n",
    "#1. Define the neural network framework: input, output, intermediate results, parameters (weights)\n",
    "#x = tf.constant([[0.7,0.9]])#x is a 2*1 matrix, which is inefficient\n",
    "#Use the placeholder mechanism to provide input data:\n",
    "#x = tf.placeholder(tf.float32,shape = (1,2),name = 'input')\n",
    "##Use the placeholder mechanism to provide multiple sample input data, such as 3*2\n",
    "x = tf.placeholder(tf.float32,shape = (3,2),name = 'input')\n",
    "\n",
    "#3*2Matrix example: tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.][-1. -1. -1.]]\n",
    "#defined parameters\n",
    "w1 = tf.Variable(tf.random_normal((2,3),stddev = 1,seed = 1))# randomly generate normal 3*2 matrix variable parameters\n",
    "w2 = tf.Variable(tf.random_normal((3,1),stddev = 1,seed = 1))#1*3 matrix\n",
    "#definition intermediate results and output\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#2. Session startup diagram\n",
    "sess = tf.Session()\n",
    "\n",
    "# variable needs to be initialized\n",
    "#sess.run(w1.initializer)\n",
    "#sess.run(w2.initializer)\n",
    "\n",
    "#3. When there are many variables, you can initialize all the variables in one sentence.\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "print (sess.run(w1))\n",
    "print (sess.run(w2))\n",
    "#If you do not use the placeholder input mechanism, run the following\n",
    "#print(sess.run(y))\n",
    "\n",
    "#4. If you use placeholder, you need to specify the value of x with a dictionary.\n",
    "print(sess.run(y,feed_dict ={x:[[0.7,0.9],[0.1,0.4],[0.5,0.8]]}))\n",
    "\n",
    "#tensorboardVisualization, you need to enter the command tensorboard --logdir logs\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "#5.Close the conversation\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-11-a705f2c40212>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-a705f2c40212>\"\u001b[1;36m, line \u001b[1;32m58\u001b[0m\n\u001b[1;33m    sess.run(train_step,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#Complete neural network example：\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "#Backpropagation algorithm steps:\n",
    "#1. Define the batch size. 2. Calculate the forward propagation algorithm to get y. 3. Calculate the cost function (y'-y).\n",
    "#4. According to the gradient descent method (need to find the partial derivative of cost versus w), \n",
    "#calculate backpropagation (update w'=w-l* gradient). 5. Update w', then observe the cost, if cost=0, end.\n",
    "#if cost!=0, but the number of trainings is reached and ends. If cost!=0, the number of trainings is not reached, the next batch+1\n",
    "\n",
    "#Part A: Definition\n",
    "#1. Define the batch size\n",
    "batch_size = 8\n",
    "\n",
    "#2. Define the neural network framework: input, output, intermediate results, parameters (weights), activation functions\n",
    "#Input: Forward + Reverse\n",
    "x = tf.placeholder(tf.float32,shape = (None,2),name = 'x-input')#None can be entered for multiple samples of any size\n",
    "y_ = tf.placeholder(tf.float32,shape = (None,1),name = 'y-input')#Correct result\n",
    "#output and intermediate results:\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)#predicted results\n",
    "#parameter\n",
    "w1 = tf.Variable(tf.random_normal((2,3),stddev = 1,seed = 1))# randomly generate normal 3*2 matrix variable parameters\n",
    "w2 = tf.Variable(tf.random_normal((3,1),stddev = 1,seed = 1))#1*3 matrix\n",
    "#defined activation function\n",
    "y = tf.sigmoid(y)\n",
    "#3. Define the loss function\n",
    "cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0))+(1-y)*tf.log(tf.clip_by_value(1-y,1e-10,1.0 )))\n",
    "#4. Define backpropagation algorithms: There are three common optimization methods: tf.train.GradientDescentOptimizer, tf.train.AdamOptimizer, and tf.train.MomentumOptimizer\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)# Minimize the loss function with optimization\n",
    "#Part B: Randomly generate a simulation data set\n",
    "rdm = RandomState(1)# Initialize a random value of 1, randomly generate an integer\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)# randomly generates a matrix of 2*128\n",
    "Y= [[int(x1+x2<1)] for (x1,x2) in X]#new_list = [expression(i) for i in old_list if filter(i)]\n",
    "#expression(i) is [int(x1+x2<1)], if x1+x2<1 is 0, if x1+x2>1 is 1, then we get a list Y consisting of 1 and 0.\n",
    "\n",
    "#Part C: Creating a conversation\n",
    "#with tf.Session() as sess:# Use the with as statement to simplify the code so that you can close the dialog without calling the Sess.close() function.\n",
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "#Print parameters before training\n",
    "print (sess.run(w1))\n",
    "print (sess.run(w2))\n",
    "\n",
    "#Training\n",
    "\n",
    "#feed data\n",
    "STEPS = 5000\n",
    "for i in range(STEPS):\n",
    "    start = (i*batch_size) % dataset_size\n",
    "    end = min(start+batch_size,dataset_size)\n",
    "# Train the network and update the parameters based on the selected samples. Backpropering what train_step does:\n",
    "#1. Calculate the forward propagation algorithm to get y. 2. Calculate the cost function (y'-y).\n",
    "#3. According to the gradient descent method (need to find the partial derivative of cost versus w), calculate backpropagation (update w'=w-l* gradient). 4. Update w', then observe the cost, if cost=0, end.\n",
    "#ifcost!=0, but the number of trainings is reached and ends. If cost!=0, the number of trainings is not reached, the next batch+1\n",
    "sess.run(train_step,\n",
    "             feed_dict={x : X[start:end],y_ : Y[start:end]})\n",
    "# Calculate cross entropy on all data at regular intervals\n",
    "    if i % 1000 == 0:\n",
    "        total_cross_entropy = sess.run(cross_entropy,\n",
    "                                       feed_dict={x: X,y_:Y})\n",
    "        print(\"After %d training step(s),cross entropy on all data is %g\"%(i,total_cross_entropy))\n",
    "print (sess.run(w1))\n",
    "print (sess.run(w2))\n",
    "\n",
    "#Close conversation\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
